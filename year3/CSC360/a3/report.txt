Design
I decided to base the design of my LLFS on the log file system described in the assignment description and from this video lecture: https://www.youtube.com/watch?v=KTCkW_6zz2k&t=3s.  In my LLFS, block 0 is the superblock and block 1 is the free block list as detailed in the assignment description.  I decided not to put the free inode list in the “checkpoint region” of the first 10 blocks and instead always append the imap at the end of the log, which simulated less seeking of the disk.  However, any time a file or directory was updated, deleted, or created, this required a seek to the free block vector in block 1 anyway.  I tracked the next_block, which pointed to the next available block to write the next content to, so the imap is always at next_block – 1.
The imap keeps track of all of the free inodes’ locations.  Any unallocated inode is represented by a 0.  I decided to make this only one block of 128 ints for simplicity, as I didn’t think it was very feasible to have close to 4096 inodes (if they were represented by bits as in the free block vector) for only 4096 data blocks.  Looking back, I could have easily doubled the number of inodes to 256 by using int16_t types.  The block number of the home inode is always at index 0 of the imap, which as the imap is persistent on disk, can always be accessed.
When a file is created, several steps take place:
    • A new inode is allocated and written to the next available block of the disk
    • The file inode (imap index + 1) is written to the parent directory data block buffer
    • The parent directory buffer is written to the next block; deallocate the old parent directory data block
    • The parent directory inode is updated (file_size and direct_blocks[0]) and written to the next block; deallocate the old parent directory inode block
    • Update imap and write to the end of the log
Conceptually, the steps are similar for creating a directory, appending to a file, and deleting a file or directory.  The key process is:
    • Allocate/update an inode
    • Update the parent directory
    • Write any new content for the specified file/directory to the disk
    • Write any new content for the parent directory to the disk
    • Update and write the imap to the disk
    • Deallocate deprecated blocks (done as new blocks are allocated)
That way, the end of the log is always guaranteed to have at least three new blocks (3 in the delete function, more for others): the parent directory data block, the parent directory inode block, and the imap.  I have submitted some images reflecting how I checked the correctness of the vdisk, which will hopefully provide better insight into how the log is written to.
Tradeoffs
By always writing the updated imap to disk, I reduce the number of seeks by 1 every time a function is performed on the disk.  I feel like this could also have easily been done with the free block vector using the same thought process.  However, I kept the free block vector in block 1 as specified in the assignment.  The tradeoff of constantly writing the imap is that the disk fills up much quicker, and that eventually, there may be lots of data scattering, leading to a large number of seeks when reading large files.  I didn’t have time to implement or consider freeing up larger chunks of blocks after the disk fills up as outlined in the video.
Since the first 10 blocks are unused by data, and I only use the first 2, theoretically this vdisk could support up to 9 * 4096 = 36 864 blocks without much more disk formatting.  However, this would make the number of inodes (128) far too small.  I noted earlier that using int16_t types in the imap would double the number of inodes to 256, but that would still be inefficient.  In that case, it would make much more sense to implement the imap as a bit vector, similar to the free block vector.  This could be done in a future iteration, but right now I was willing to accept the tradeoff of a low number of inodes as the size of the vdisk is currently unlikely to require that much more space.
I don’t actually maintain a copy of the free block vector in memory, I just read and write to the first block every time I allocate or deallocate a block.  This is much slower, and requires lots of seeks, but ensures that the free block vector is updated and persistent.
Robustness Considerations
I did not have time to work on goal 3, making the LLFS robust.  However, I think that my design lends itself quite well to being robust in some areas, but not in others.  The imap is written to after every update of an inode.  The imap in memory could thus be easily copied from the last block of the disk.  A solution to check if the imap is in fact the last block in the log could be to implement a magic number, checking for the first byte.  If it is not found, then it can be assumed that the last imap is not correct.  Then, a fsck program could iterate through the allocated blocks following the last imap.  Inodes, files, and directories are all clearly distinguished already, so the fsck could just compare the latest inodes with the imap entries, and the files and directory blocks with the inodes and the free block vector.  
To test the fsck capability of my program, I would probably create new test-only functions that implemented much of the same functionality as creation, deletion, and updating, but which exited at different points.  I would call these functions in the same order as I had in the non-crash tests.  Then I would run the fsck function, and compare the vdisk contents with the contents from the non-crash tests.
Known Bugs and Defects
Refer to README.md.
